version: "3.4"
services:
  weaviate:
    command: ["--host", "0.0.0.0", "--port", "8080", "--scheme", "http"]
    image: semitechnologies/weaviate:1.23.9
    ports:
    - 9090:8080
    - 50050:50051
    volumes:
    - /home/minh/Downloads/law-data/weaviate_data:/var/lib/weaviate
    restart: always
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DISK_USE_READONLY_PERCENTAGE: 100
      DISK_USE_WARNING_PERCENTAGE: 100
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'

  embeddings:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    command: ["--model-id", "BAAI/bge-m3"]
    volumes:
      - /home/minh/Downloads/data:/data
    ports:
      - 8081:80
    environment:
      HUGGING_FACE_HUB_TOKEN: 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

  gptcache:
    image: gptcache:v0.0
    volumes:
    - ./gptcache.yml:/workspace/gptcache.yml
    ports:
    - 8080:8000
    command: ["gptcache_server", "-s", "0.0.0.0", "-p", "8000", "-f", "gptcache.yml"]

  

  # legal-rag:
  #   build:
  #     dockerfile: Dockerfile.external
  #   image: legal-rag
  #   volumes:
  #     - ./local_data/:/home/worker/app/local_data
  #   # network_mode: "host"    
  #   ports:
  #     - 8001:8080
  #   depends_on:
  #     - embeddings
  #     - weaviate
  #   environment:
  #     PORT: 8080
  #     PGPT_PROFILES: docker
  #     LLM: vllm
  #     EMBEDDING: text_embeddings_inference
  #     VECTOR_STORE: weaviate
  #     VLLM_ENDPOINT: http://103.145.79.20:6017/v1/completions
  #     TEXT_EMBEDDINGS_INFERENCE_ENDPOINT: http://embeddings:80
  #     WEAVIATE_ENDPOINT: http://weaviate:8080

  # rerank:
  #   image: rerank:v1.0
  #   volumes:
  #     - /home/duongntd/.cache/huggingface/hub/.locks/models--BAAI--bge-reranker-base:/home/api/.cache/huggingface/hub/.locks/models--BAAI--bge-reranker-base
  #     - /home/duongntd/.cache/huggingface/hub/models--BAAI--bge-reranker-base:/home/api/.cache/huggingface/hub/models--BAAI--bge-reranker-base
  #   entrypoint: >
  #     uvicorn main:app --host 0.0.0.0 --port 8084 --log-level info --workers=1
  #   ports:
  #     - 8084:8084
  #   shm_size: 7gb
    # environment:
    #   MODEL_NAME: BAAI/bge-reranker-base
  # translation:
  #   image: envit5_translation:v0.5
  #   entrypoint: 
  #     uvicorn main:app --host 0.0.0.0 --port 8085 --log-level info --workers=4
  #   ports:
  #     - 8085:8085
  #   shm_size: 7gb